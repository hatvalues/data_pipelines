# build
docker build . -t datpip:latest
sudo docker build . -t datpip:latest

# For running the containers:
# NOTE: this line:

# Linux Machine
# --volume $HVSNDBOXSVC:/credentials.json:ro \

# XPX Mac
# --volume $DS_SERVICE_KEY:/credentials.json:ro \

# the environment variable is... (to be aligned across both machines)
# a string literal of the full path to the system account private key on the host machine

# NOTE: the volumes set up e.g. this line:
#  --volume "$(pwd)":/usr/src/app \
# is there to make dev easier because don't have to build after changes
# but should not be included when performing final stage (preprod) testing

# LINUX

# use sudo if local Docker folder not writable by current user
docker build . -t datpip:latest

sudo docker run -it --rm --name datpip \
  --volume $HVSNDBOXDSSVC:/credentials.json:ro \
  --env GOOGLE_APPLICATION_CREDENTIALS=/credentials.json \
  --env PROJECT_ID=hatvalues-sandbox \
  --env DATASET=datalake \
  --env BUCKET_NAME=data-ingest-bk \
  --volume "$(pwd)":/usr/src/app \
  --volume "$(pwd)/logs":/tmp \
  --volume "$(pwd)/data":/usr/src/app/data \
  datpip:latest

# Mac
docker build . -t datpip:latest

docker run -it --rm --name datpip \
  --volume $FLOWS_SERVICE_KEY:/credentials.json:ro \
  --env GOOGLE_APPLICATION_CREDENTIALS=/credentials.json \
  --env PROJECT_ID=hatvalues-sandbox \
  --env DATASET=datalake \
  --env BUCKET_NAME=data-ingest-bk \
  --volume "$(pwd)":/usr/src/app \
  --volume "$(pwd)/logs":/tmp \
  --volume "$(pwd)/data":/usr/src/app/data \
  datpip


# WINDOWS
docker build . -t datpip:latest

docker run -it --rm --name datpip ^
--env DATASTORE_PROJECT_ID=hatvalues-sandbox ^
--env DATASTORE_DATASET=hatvalues-sandbox ^
--volume %DS_SERVICE_KEY%:/credentials.json:ro ^
--env GOOGLE_APPLICATION_CREDENTIALS=/credentials.json ^
--volume %CD%:/usr/src/app ^
--volume %CD%/logs:/tmp ^
datpip
